â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             BRONZE LEVEL LOADER                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             INITIALIZATION PHASE                              â”‚
â”‚                                                                               â”‚
â”‚ 1. Set up logging system                                                     â”‚
â”‚ 2. Initialize Spark session                                                  â”‚
â”‚ 3. Initialize database connection                                            â”‚
â”‚ 4. Load predefined schemas                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               DATA LOADING PHASE                              â”‚
â”‚                                                                               â”‚
â”‚ 1. Read CSV files with fallback mechanisms                                   â”‚
â”‚ 2. Perform column case analysis                                              â”‚
â”‚ 3. Standardize column names                                                  â”‚
â”‚ 4. Preprocess data (type conversion, cleaning)                               â”‚
â”‚ 5. Convert to Spark DataFrames                                               â”‚
â”‚ 6. Add metadata columns                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA PERSISTENCE PHASE                           â”‚
â”‚                                                                               â”‚
â”‚ 1. Save to Parquet files (Bronze layer storage)                              â”‚
â”‚ 2. Save to PostgreSQL database                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA EXTRACTION PHASE                            â”‚
â”‚                                                                               â”‚
â”‚ 1. Filter data by year                                                       â”‚
â”‚ 2. Return filtered datasets                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ—ï¸ BRONZE LEVEL LOADER WORKFLOW
ğŸ”° INITIALIZATION PHASE
ğŸ§© Setting the stage for the pipeline

1ï¸âƒ£ ğŸ“ Set up structured logging â€“ Ensures all actions are traceable and well-documented
2ï¸âƒ£ âš™ï¸ Initialize Spark session â€“ Powers distributed data processing
3ï¸âƒ£ ğŸ”— Connect to the PostgreSQL database â€“ Establishes a persistent storage channel
4ï¸âƒ£ ğŸ“œ Load predefined schemas â€“ Brings schema consistency across datasets

â¬‡ï¸

ğŸ“¥ DATA LOADING PHASE
ğŸ”„ Where raw data starts to take shape

1ï¸âƒ£ ğŸ“‚ Read CSV files with fallback mechanisms â€“ Resilient ingestion even under irregular formats
2ï¸âƒ£ ğŸ§  Analyze column casing â€“ Identifies naming pattern issues
3ï¸âƒ£ âœï¸ Standardize column names â€“ Enforces naming consistency
4ï¸âƒ£ ğŸ§¹ Preprocess data â€“ Handles type conversions and cleans dirty values
5ï¸âƒ£ ğŸ”„ Convert to Spark DataFrames â€“ Enables scalable processing
6ï¸âƒ£ ğŸ·ï¸ Add metadata columns â€“ Tracks lineage and processing info

â¬‡ï¸

ğŸ›¢ï¸ DATA PERSISTENCE PHASE
ğŸ’¾ Storing the transformed data

1ï¸âƒ£ ğŸ“¦ Save to Parquet (Bronze Layer) â€“ Efficient columnar storage for future processing
2ï¸âƒ£ ğŸ—ƒï¸ Save to PostgreSQL database â€“ Makes data available for structured querying

â¬‡ï¸

ğŸ¯ DATA EXTRACTION PHASE
ğŸ” Accessing the refined data

1ï¸âƒ£ ğŸ“† Filter by year â€“ Extracts time-specific subsets
2ï¸âƒ£ ğŸ“¤ Return filtered datasets â€“ Ready for analysis or promotion to Silver layer

ğŸŒŸ Function Explanations
ğŸ”§ Initialization Functions
__init__:
ğŸ› ï¸ Initializes the loader with logging, Spark session, DB connection, and schema configurations.

_setup_logger:
ğŸ“‹ Configures structured logging with both file and console handlers for transparent monitoring.

ğŸ“Š Data Analysis Functions
_analyze_column_case:
ğŸ” Analyzes column naming conventions and flags inconsistencies or formatting issues.

_log_dataframe_stats:
ğŸ“ˆ Provides comprehensive statistics and insights about DataFrames (e.g., row count, null values).

ğŸ§¹ Data Processing Functions
_preprocess_data:
ğŸ§¼ Performs data cleaning and ensures correct type conversion for seamless processing.

_read_csv_with_fallback:
ğŸ”„ A robust CSV reader with smart fallback strategies in case of read failures or format variations.

ğŸš€ Core Loading Functions
load_bronze_data:
ğŸ§© Orchestrates the complete data loading process from raw to bronze stage with validations and logging.

_save_to_postgres:
ğŸ—ƒï¸ Saves processed data to a PostgreSQL database while ensuring detailed traceability and logging.

ğŸ§° Utility Functions
get_case_analysis:
ğŸ§  Returns results from column naming pattern analysis to aid in schema consistency checks.

extract_year_data:
ğŸ“† Filters the dataset to include only records from a specified year.

ğŸ¯ Main Function
main:
ğŸ§­ The central entry point coordinating the full ETL workflow, from ingestion to persistence.